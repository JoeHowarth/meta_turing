{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend you take a look at these material first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture13-CNNs.pdf\n",
    "* http://www.aclweb.org/anthology/D14-1181\n",
    "* https://github.com/Shawn1993/cnn-text-classification-pytorch\n",
    "* http://cogcomp.org/Data/QA/QC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "import re\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "from data_utils_discrim import Dictionary, Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000   # number of words to be sampled\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Penn Treebank Dataset\n",
    "path_real = './language_model_basic_lstm/data/train.txt'\n",
    "path_fake = './language_model_basic_lstm/data/sample.txt'\n",
    "\n",
    "corpus = Corpus()\n",
    "ids_real = corpus.get_data(path_real, batch_size)\n",
    "print(len(corpus.dictionary))\n",
    "ids_fake = corpus.get_data(path_fake, batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)\n",
    "num_batches = ids_fake.size(1) // seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path_real, 'r') as f:\n",
    "#     x = f.readlines()\n",
    "# with open(path_fake, 'r') as f:\n",
    "#     y = f.readlines()\n",
    "    \n",
    "# print(\"fake\",len(y),\"real\", len(x))\n",
    "print(ids_real.shape, ids_fake.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    x_p=[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([word2index['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p),torch.cat(y).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ids_fake.shape)\n",
    "[1]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word2index = corpus.dictionary.word2idx\n",
    "index2word = corpus.dictionary.idx2word\n",
    "\n",
    "ids_real_used = ids_real[:ids_fake.shape[0]]\n",
    "paired_real = zip(ids_real_used, [1]*ids_real_used.shape[0])\n",
    "paired_fake = zip(ids_fake, [1]*ids_fake.shape[0])\n",
    "\n",
    "data = torch.cat([ids_fake, ids_real_used], 0)\n",
    "print(data.shape)\n",
    "print(list(paired_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "data_p = list(zip(X_p,y_p))\n",
    "random.shuffle(data_p)\n",
    "\n",
    "train_data = data_p[:int(len(data_p)*0.9)]\n",
    "test_data = data_p[int(len(data_p)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can download pretrained word vector from here https://github.com/mmihaltz/word2vec-GoogleNews-vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embed_model():\n",
    "    from gensim.models import KeyedVectors\n",
    "    # load the Stanford GloVe model\n",
    "    filename = 'data/glove.6B.100d.txt.word2vec'\n",
    "    embed_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "    return embed_model\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('../dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = load_embed_model()\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('../dataset/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    try:\n",
    "        pretrained.append(model[word2index[i]])\n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/08.cnn-for-text-architecture.png\">\n",
    "<center>borrowed image from http://www.aclweb.org/anthology/D14-1181</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embedding_dim,output_size,kernel_dim=100,kernel_sizes=[3,4,5],dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self,pretrained_word_vectors=False,is_static=False):\n",
    "        \n",
    "        if pretrained_word_vectors:\n",
    "            self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        else: \n",
    "            self.embedding.weight.uniform_(-.1,.1)\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs,is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes for a while if you use just cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=5\n",
    "BATCH_SIZE=50\n",
    "KERNEL_SIZES = [3,4,5]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CNNClassifier(len(word2index),300,len(target2index),KERNEL_DIM,KERNEL_SIZES)\n",
    "# model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "model.init_weights() # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs,True)\n",
    "        \n",
    "        loss = loss_function(preds,targets)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test in test_data:\n",
    "    pred = model(test[0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    target = test[1].data.tolist()[0][0]\n",
    "    if pred == target:\n",
    "        accuracy+=1\n",
    "\n",
    "print(accuracy/len(test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://arxiv.org/pdf/1508.06615.pdf\">Character-Aware Neural Language Models</a>\n",
    "* <a href=\"https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\">Character level CNN for text classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278\n",
    "* <a href=\"https://arxiv.org/pdf/1607.01759\">Bag of Tricks for Efficient Text Classification</a>\n",
    "* <a href=\"https://arxiv.org/pdf/1708.02657\">Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
