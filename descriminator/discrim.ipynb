{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk import ngrams, RegexpTokenizer, FreqDist\n",
    "\n",
    "\n",
    "import random, numpy as np, re\n",
    "from collections import Counter, OrderedDict\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def glove_to_word2vec():\n",
    "    from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "    glove_input_file = 'data/glove/glove.6B.300d.txt'\n",
    "    word2vec_output_file = 'data/glove.6B.300d.txt.word2vec'\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = brown.sents()\n",
    "sents = [\" \".join(x) for x in sents]\n",
    "\n",
    "def tokens2ids(sent, word_to_ix, max_sent_len=50):\n",
    "    ids = [word_to_ix[x] for x in sent]\n",
    "    while len(ids) < max_sent_len:\n",
    "        ids.append(900800700)\n",
    "    return np.asarray(ids)\n",
    "\n",
    "def getTrain(sents):\n",
    "    tokenizer = RegexpTokenizer(r'[a-z][a-z\\']*').tokenize\n",
    "    t_sents = [tokenizer(sent.lower()) for sent in sents]\n",
    "    t_sents = [t for t in t_sents if len(t) < 40]\n",
    "\n",
    "    vocab = set([x for sent in t_sents for x in sent])\n",
    "\n",
    "    word_to_ix['<PAD>'] = 0 \n",
    "    word_to_ix['<UNK>'] = 1\n",
    "    word_to_ix = {word: i+2 for i, word in enumerate(vocab)}\n",
    "    \n",
    "    id_sents = np.asarray([tokens2ids(x, word_to_ix) for x in t_sents])\n",
    "    return id_sents, vocab, word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_sents, vocab, word2index = getTrain(sents)\n",
    "train_data = id_sents\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    x_p=[]\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1)<max_x:\n",
    "            x_p.append(torch.cat([x[i],Variable(LongTensor([word2index['<PAD>']]*(max_x-x[i].size(1)))).view(1,-1)],1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p),torch.cat(y).view(-1)\n",
    "\n",
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if w in to_index.keys() else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embedding_dim,output_size,kernel_dim=100,kernel_sizes=[3,4,5],dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*kernel_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def init_weights(self,pretrained_word_vectors,is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs,is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1) # (B,1,T,D)\n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs] #[(N,Co,W), ...]*len(Ks)\n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated) \n",
    "        return F.log_softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec_path = \"\"\n",
    "def load_embed_model():\n",
    "    from gensim.models import KeyedVectors\n",
    "    # load the Stanford GloVe model\n",
    "    filename = 'data/glove.6B.100d.txt.word2vec'\n",
    "    embed_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "    return embed_model\n",
    "\n",
    "# model = KeyedVectors.load_word2vec_format('../dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model = load_embed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    try:\n",
    "        pretrained.append(model[word2index[i]])\n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "pretrained_vectors = np.vstack(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCH=5\n",
    "BATCH_SIZE=50\n",
    "KERNEL_SIZES = [3,4,5]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CNNClassifier(len(word2index),300,len(target2index),KERNEL_DIM,KERNEL_SIZES)\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs,True)\n",
    "        \n",
    "        loss = loss_function(preds,targets)\n",
    "        losses.append(loss.data.tolist()[0])\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100==0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch,EPOCH,np.mean(losses)))\n",
    "            losses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "for test in test_data:\n",
    "    pred = model(test[0]).max(1)[1]\n",
    "    pred = pred.data.tolist()[0]\n",
    "    target = test[1].data.tolist()[0][0]\n",
    "    if pred == target:\n",
    "        accuracy+=1\n",
    "\n",
    "print(accuracy/len(test_data)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-z][a-z\\']*').tokenize\n",
    "# tokenized = [tokenizer(rev) for rev in train]\n",
    "\n",
    "def sent2embed(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    embed_matrix = np.asarray([embed_model[token] for token in tokens])\n",
    "    \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = sent2embed(\"this is my life\")\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
